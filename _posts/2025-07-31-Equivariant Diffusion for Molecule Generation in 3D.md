---
title: "Equivariant Diffusion for Molecule Generation in 3D"
date: 2025-07-31
permalink: /blog/2025-07-31-Equivariant-Diffusion-for-Molecule-Generation-in-3D
---

# Equivariant Diffusion for Molecule Generation in 3D

*By Arjit Yadav*

---

### üß† Introduction: Deep Learning Meets Molecular Sciences

The intersection of artificial intelligence and chemistry is revolutionizing drug discovery, materials design, and protein modeling. Among these breakthroughs, *3D molecular generation* stands out as a key challenge. Traditional models mostly work with 2D molecular graphs, omitting essential geometric information. But molecules are inherently three-dimensional ‚Äî and geometry matters.

**Enter: Equivariant Diffusion Models (EDMs).**

This blog post explores how EDMs leverage the symmetries of 3D space (translation, rotation, and reflection) to generate stable, realistic molecular structures. Based on the paper ["Equivariant Diffusion for Molecule Generation in 3D"](https://arxiv.org/abs/2203.17003) by Hoogeboom et al., we dive into the architecture, training strategy, and results of EDM.

---

## üìå Table of Contents

1. [The Challenge of 3D Molecular Generation](#1-the-challenge-of-3d-molecular-generation)
2. [Geometry and Symmetries in Molecules](#2-geometry-and-symmetries-in-molecules)
3. [From Noise to Molecule: Diffusion Models](#3-from-noise-to-molecule-diffusion-models)
4. [Equivariance and Invariance Explained](#4-equivariance-and-invariance-explained)
5. [Data Representation: Atoms as Points and Features](#5-data-representation-atoms-as-points-and-features)
6. [Building Blocks of EDM](#6-building-blocks-of-edm)
7. [The Equivariant Graph Neural Network (EGNN)](#7-the-equivariant-graph-neural-network-egnn)
8. [Training Objective and Optimization](#8-training-objective-and-optimization)
9. [Handling Atom Types and Molecule Size](#9-handling-atom-types-and-molecule-size)
10. [Experimental Setup and Datasets](#10-experimental-setup-and-datasets)
11. [Results and Visualizations](#11-results-and-visualizations)
12. [Strengths, Limitations, and Applications](#12-strengths-limitations-and-applications)
13. [Conclusion](#13-conclusion)

---

## 1. The Challenge of 3D Molecular Generation

Most generative models treat molecules as graphs‚Äînodes for atoms and edges for bonds. However, these ignore 3D conformations that are critical for a molecule‚Äôs function and interactions.

> *Generating accurate 3D structures means ensuring chemical validity, geometric realism, and symmetry awareness ‚Äî all at once.*

Traditional models:
- Are limited to 2D graphs
- Ignore spatial constraints
- Can‚Äôt represent conformations directly

EDMs tackle this by operating directly in 3D Euclidean space and using **diffusion processes**.

---

## 2. Geometry and Symmetries in Molecules

Molecules obey geometric rules:
- **Translation:** Shift all atoms
- **Rotation:** Rotate the molecule
- **Reflection:** Mirror it across a plane

These transformations form the **E(3) group**. Any physical model should respect these symmetries.

### Why does this matter?
A model that learns on a molecule in one orientation should perform identically on any rotated version.

This leads us to **equivariance** and **invariance**.

---

## 3. From Noise to Molecule: Diffusion Models

Diffusion models learn data distributions by *reversing a noise process*. 

**Forward process:** Add Gaussian noise to data ‚Üí z<sub>T</sub>

**Reverse process:** Learn to denoise ‚Üí xÃÇ, hÃÇ (coordinates & atom types)

<p align="center">
  <img src="https://user-images.githubusercontent.com/61483733/160250004-6f657871-9499-4a01-8590-2d52dfd35760.png" width="500"/>
</p>

> At each step t, the network learns to denoise z<sub>t</sub> to get closer to the true molecule.

---

## 4. Equivariance and Invariance Explained

- **Equivariance:** If input is transformed, output transforms accordingly.
  - f(Rx) = Rf(x)

- **Invariance:** The output distribution doesn‚Äôt change under transformations.
  - p(x) = p(Rx)

In EDM:
- The noise is added to a **zero center of gravity** subspace (to handle translation)
- The denoising network is **rotation-equivariant** (using EGNNs)

---

## 5. Data Representation: Atoms as Points and Features

Each molecule is represented by:
- **x ‚àà ‚Ñù<sup>M√ó3</sup>**: 3D coordinates of M atoms
- **h ‚àà ‚Ñù<sup>M√ónf</sup>**: Features like atom type (one-hot), charge

During training:
- M (number of atoms) is sampled from the data
- Noise is added to both x and h

The model learns to recover clean positions and correct atom types.

---

## 6. Building Blocks of EDM

The EDM architecture contains:
- A **diffusion process** over positions and features
- A **denoising network** œÜ (approximates noise)
- An **equivariant graph neural network (EGNN)** to predict noise

During inference:
1. Sample z<sub>T</sub> from standard normal
2. Iteratively denoise using œÜ
3. Output (x, h) as final molecule

---

## 7. The Equivariant Graph Neural Network (EGNN)

EGNN layers:
- Respect E(n) equivariance (rotation, translation)
- Update atom features and positions using neighbors
- Operate on fully connected graphs (every atom interacts)

Update rules:
```math
m_{ij} = œÜ_e(h_i, h_j, ||x_i - x_j||^2)\
h_i' = œÜ_h(h_i, \sum_j m_{ij})\
x_i' = x_i + \sum_j \frac{x_i - x_j}{||x_i - x_j|| + 1} œÜ_x(...)
```

All functions œÜ are MLPs.

---

## 8. Training Objective and Optimization

EDM is trained to minimize the noise prediction error:

```math
L_t = \mathbb{E}[||Œµ - œÜ(z_t, t)||^2]
```

The full objective includes:
- Noise prediction loss (main term)
- Likelihood terms for atom types and coordinates

We set weighting term w(t) = 1 for stability.

---

## 9. Handling Atom Types and Molecule Size

- **Atom types:** One-hot encoding + categorical diffusion
- **Charges:** Integer representation with integrated normal likelihood
- **Scaling:** Features scaled down (e.g., 0.25√ó for one-hot, 0.1√ó for charge)
- **Number of atoms (M):** Sampled from p(M) learned from data

---

## 10. Experimental Setup and Datasets

### QM9
- ~130k molecules
- Small, up to 9 heavy atoms
- Used for unconditional and conditional generation

### GEOM-Drugs
- 430k molecules
- Larger, drug-like molecules
- Average 44 atoms

---

## 11. Results and Visualizations

### üß™ QM9 Results
| Model | NLL | Atom Stable (%) | Molecule Stable (%) |
|-------|-----|------------------|----------------------|
| EDM  | -110.7 ¬±1.5 | 98.7 ¬±0.1 | 82.0 ¬±0.4 |

### ‚úÖ Validity and Uniqueness
| Model | Valid (%) | Unique (%) |
|-------|-----------|-------------|
| EDM (no H) | 91.9 | 90.7 |
| EDM (with H) | 97.5 | 94.3 |

<p align="center">
  <img src="https://github.com/yourusername/images/qm9_geom_edm_samples.png" width="500" />
</p>

### üíä GEOM-Drugs Results
| Model | NLL | Atom Stability | Wasserstein Distance |
|-------|-----|----------------|-----------------------|
| EDM   | -137.1 | 81.3% | 1.41 |

---

## 12. Strengths, Limitations, and Applications

‚úÖ **Strengths:**
- E(3) equivariance: better generalization
- Joint modeling of geometry and atom types
- No need for atom ordering
- Probabilistic: can compute likelihoods
- High-quality, stable molecules

‚ö†Ô∏è **Limitations:**
- Sampling speed is slower
- Occasional disconnected structures
- Conditional generation has room to improve

üî¨ **Applications:**
- Drug discovery
- Materials design
- Protein‚Äìligand modeling

---

## 13. Conclusion

Equivariant Diffusion Models represent a powerful leap in 3D molecular generation. By respecting the fundamental symmetries of physics, EDMs produce stable, realistic molecular structures with a principled probabilistic foundation.

As research progresses, we expect faster sampling and even better conditional control ‚Äî paving the way toward generative AI for molecular design.

---

üìö **References**
- [Hoogeboom et al., 2022 (arXiv:2203.17003)](https://arxiv.org/abs/2203.17003)
- [EGNN: Satorras et al., 2021](https://arxiv.org/abs/2102.09844)

---

üéì *Presented by Arjit Yadav (3691856)*

> For suggestions or contributions, feel free to open a pull request!
